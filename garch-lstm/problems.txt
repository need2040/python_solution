Градиенты атакуют omega (см. Numerical underflow) - потенциальное решение - умножение на 100, а потом деление (обьяснить почему) 
RNN схлопывается в linear слой для garch(1,1)
При предсказании константных параметров и большой выборке лагирование не значительно отклоняет данные (как-то проверить?)