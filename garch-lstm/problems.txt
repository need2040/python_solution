Градиенты атакуют omega (см. Numerical underflow) - потенциальное решение - умножение на 100, а потом деление (обьяснить почему) 
RNN схлопывается в linear слой для garch(1,1)
При предсказании константных параметров и большой выборке лагирование не значительно отклоняет данные (как-то проверить?)
Отрицательные веса влекут за собой проблемы (!!!!) - де-факто легко исправит 
Для того, что бы исправить nan значения можно инициализировать веса, как .2,.2,.2 (или в целом строго положительные - обьяснить почему adam не справляется иначе) 
NLLLoss требует намного большей выборки
Для модели чем больше омега, тем лучше 
Sign function в gjr-garch на самом деле индикатор [x<0], (см.gjr-garch)
Де-факто получается так, что nn-модель может угадывать параметры, при которых ARCH модели расходятся (может ли быть такое на практике?)