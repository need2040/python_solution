Градиенты атакуют omega (см. Numerical underflow) - потенциальное решение - умножение на 100, а потом деление (обьяснить почему) 
RNN схлопывается в linear слой для garch(1,1)
При предсказании константных параметров и большой выборке лагирование не значительно отклоняет данные (как-то проверить?)
Отрицательные веса влекут за собой проблемы (!!!!) - де-факто легко исправит 
Для того, что бы исправить nan значения можно инициализировать веса, как .2,.2,.2
NLLLoss требует намного большей выборки

Для модели чем больше омега, тем лучше 

