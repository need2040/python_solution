{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7919932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5d5d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qpsk_augmentation(x):\n",
    "    x = x.clone()\n",
    "    x += torch.randn_like(x) * 0.05  # white noise\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        phase_shift = 2 * np.pi * torch.rand(1).item()\n",
    "        rotation = torch.tensor([\n",
    "            [np.cos(phase_shift), -np.sin(phase_shift)],\n",
    "            [np.sin(phase_shift),  np.cos(phase_shift)]\n",
    "        ], dtype=torch.float32, device=x.device)\n",
    "        x = rotation @ x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93ea41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPSKDataset(Dataset):\n",
    "    def __init__(self, num_samples, time, symbol_rate, sample_rate, carrying_freq, augment_fn):\n",
    "        self.num_samples = num_samples\n",
    "        self.time = time\n",
    "        self.symbol_rate = symbol_rate\n",
    "        self.sample_rate = sample_rate\n",
    "        self.carrying_freq = carrying_freq\n",
    "        self.augment_fn = augment_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def generate_qpsk(self):\n",
    "        num_symbols = int(self.time * self.symbol_rate)\n",
    "        num_samples = int(self.time * self.sample_rate)\n",
    "        samples_per_symbol = int(self.sample_rate / self.symbol_rate)\n",
    "\n",
    "        bits = np.random.randint(0, 2, size=num_symbols * 2)\n",
    "        symbols = bits.reshape(-1, 2)\n",
    "\n",
    "        phase_map = {\n",
    "            (0, 0): (1, 1),\n",
    "            (0, 1): (-1, 1),\n",
    "            (1, 1): (-1, -1),\n",
    "            (1, 0): (1, -1)\n",
    "        }\n",
    "\n",
    "        iq = np.array([phase_map[tuple(b)] for b in symbols])\n",
    "        i_vals, q_vals = iq[:, 0], iq[:, 1]\n",
    "\n",
    "        i_samples = np.repeat(i_vals, samples_per_symbol)\n",
    "        q_samples = np.repeat(q_vals, samples_per_symbol)\n",
    "\n",
    "        t = np.linspace(0, self.time, int(self.time * self.sample_rate), endpoint=False)\n",
    "\n",
    "        carrier_cos = np.cos(2 * np.pi * self.carrying_freq * t) * (np.sqrt(2)/2)\n",
    "        carrier_sin = np.sin(2 * np.pi * self.carrying_freq * t) * (np.sqrt(2)/2)\n",
    "\n",
    "        signal = i_samples * carrier_cos - q_samples * carrier_sin\n",
    "        iq_signal = np.stack([i_samples, q_samples], axis=0)  # Shape: [2, T]\n",
    "        return iq_signal.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean = self.generate_qpsk()\n",
    "        noisy_1 = self.augment_fn(torch.tensor(clean.copy()))\n",
    "        noisy_2 = self.augment_fn(torch.tensor(clean.copy()))\n",
    "        return noisy_1, noisy_2\n",
    "\n",
    "\n",
    "class QPSKDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=4, **signal_params):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.signal_params = signal_params\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = QPSKDataset(\n",
    "            num_samples=10000,\n",
    "            augment_fn=qpsk_augmentation,\n",
    "            **self.signal_params\n",
    "        )\n",
    "        self.val_dataset = QPSKDataset(\n",
    "            num_samples=1000,\n",
    "            augment_fn=qpsk_augmentation,\n",
    "            **self.signal_params\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=self.num_workers\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2e20242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels=2, hidden_dim=64, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_dim, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.mu_head = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x).squeeze(-1)\n",
    "        mu = self.mu_head(h)\n",
    "        logvar = self.logvar_head(h)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "class Denoiser(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.mlp(z)\n",
    "        norm = torch.norm(out, dim=-1, keepdim=True) + 1e-8\n",
    "        return out / norm  # Ensure output is on unit circle\n",
    "\n",
    "\n",
    "class QPSKDenoiserVSSL(pl.LightningModule):\n",
    "    def __init__(self, ema_decay=0.99, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.student = Encoder()\n",
    "        self.teacher = Encoder()\n",
    "        self.denoiser = Denoiser()\n",
    "        self.ema_decay = ema_decay\n",
    "        self.lr = lr\n",
    "        self.register_buffer(\"global_step_float\", torch.tensor(0.0))\n",
    "\n",
    "        # Initialize teacher with student weights\n",
    "        self._update_teacher(0.0)\n",
    "\n",
    "    def _update_teacher(self, decay=None):\n",
    "        decay = self.ema_decay if decay is None else decay\n",
    "        for t_param, s_param in zip(self.teacher.parameters(), self.student.parameters()):\n",
    "            t_param.data = decay * t_param.data + (1.0 - decay) * s_param.data\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.student(x)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        return self.denoiser(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2 = batch  # each shape: [B, 2, T]\n",
    "\n",
    "        # Teacher (no grad)\n",
    "        with torch.no_grad():\n",
    "            mu1, logvar1 = self.teacher(x1)\n",
    "        p_mu, p_logvar = mu1, logvar1\n",
    "\n",
    "        # Student\n",
    "        mu2, logvar2 = self.student(x2)\n",
    "        z = reparameterize(mu2, logvar2)\n",
    "\n",
    "        # Denoise\n",
    "        z_hat = self.denoiser(z)\n",
    "\n",
    "        # KL between Gaussians\n",
    "        kl = 0.5 * torch.sum(\n",
    "            torch.exp(logvar2 - p_logvar)\n",
    "            + (mu2 - p_mu) ** 2 / torch.exp(p_logvar)\n",
    "            - 1 + p_logvar - logvar2,\n",
    "            dim=1\n",
    "        ).mean()\n",
    "\n",
    "        # Likelihood: want denoised z_hat â‰ˆ z sampled from prior\n",
    "        with torch.no_grad():\n",
    "            z_target = reparameterize(p_mu, p_logvar)\n",
    "            z_target = z_target / (torch.norm(z_target, dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        recon_loss = F.mse_loss(z_hat, z_target)\n",
    "\n",
    "        # Phase derivative regularization\n",
    "        phase = torch.atan2(z_hat[:,1], z_hat[:,0])  # shape: [B]\n",
    "        phase_diff = torch.diff(phase)\n",
    "        phase_smoothness = (phase_diff**2).mean()\n",
    "\n",
    "        constellation_error = self._calculate_constellation_error(z_hat)\n",
    "\n",
    "        loss = kl + recon_loss + 0.1 * phase_smoothness + 0.1 * constellation_error\n",
    "\n",
    "        self.log_dict({\n",
    "            \"train/loss\": loss,\n",
    "            \"train/kl\": kl,\n",
    "            \"train/recon\": recon_loss,\n",
    "            \"train/phase_smoothness\": phase_smoothness,\n",
    "            \"train/constellation_error\": constellation_error\n",
    "        }, prog_bar=True)\n",
    "\n",
    "\n",
    "        # EMA update\n",
    "        self._update_teacher()\n",
    "        self.global_step_float += 1.0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "    def _calculate_constellation_error(self, z_hat):\n",
    "        \"\"\"Calculate how close points are to ideal QPSK constellation points\"\"\"\n",
    "        z_norm = z_hat / (torch.norm(z_hat, dim=-1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        ideal_points = torch.tensor([[1, 1], [-1, 1], [-1, -1], [1, -1]], \n",
    "                                  dtype=z_hat.dtype, device=z_hat.device) * (np.sqrt(2)/2)\n",
    "        \n",
    "        distances = torch.cdist(z_norm, ideal_points)\n",
    "        min_distances = distances.min(dim=1)[0]\n",
    "        \n",
    "        return min_distances.mean()\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1, x2 = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            clean_mu, _ = self.teacher(x1)\n",
    "            clean_z = reparameterize(clean_mu, torch.zeros_like(clean_mu))\n",
    "            clean_z = clean_z / (torch.norm(clean_z, dim=-1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            mu, logvar = self.student(x2)\n",
    "            z = reparameterize(mu, logvar)\n",
    "            z_hat = self.denoiser(z)\n",
    "            z_hat = z_hat / (torch.norm(z_hat, dim=-1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            mse = F.mse_loss(z_hat, clean_z)\n",
    "            phase_error = self._phase_error(z_hat, clean_z)\n",
    "            constellation_error = self._calculate_constellation_error(z_hat)\n",
    "            \n",
    "            self.log_dict({\n",
    "                \"val/mse\": mse,\n",
    "                \"val/phase_error\": phase_error,\n",
    "                \"val/constellation_error\": constellation_error\n",
    "            }, prog_bar=True)\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                self._log_constellation(z_hat[:16], clean_z[:16])\n",
    "    \n",
    "    def _phase_error(self, pred, target):\n",
    "        pred_phase = torch.atan2(pred[:,1], pred[:,0])\n",
    "        target_phase = torch.atan2(target[:,1], target[:,0])\n",
    "        phase_diff = torch.atan2(torch.sin(pred_phase - target_phase), \n",
    "                               torch.cos(pred_phase - target_phase))\n",
    "        return torch.abs(phase_diff).mean()\n",
    "    \n",
    "    def _log_constellation(self, pred, target):\n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "        \n",
    "        circle = plt.Circle((0, 0), 1, fill=False, linestyle='--', alpha=0.3)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        ideal_points = np.array([[1,1], [-1,1], [-1,-1], [1,-1]]) * (np.sqrt(2)/2)\n",
    "        ax.scatter(ideal_points[:,0], ideal_points[:,1], c='g', marker='x', label='Ideal')\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        target = target.detach().cpu().numpy()\n",
    "        ax.scatter(target[:,0], target[:,1], c='b', alpha=0.5, label='Clean')\n",
    "        ax.scatter(pred[:,0], pred[:,1], c='r', alpha=0.5, label='Denoised')\n",
    "        \n",
    "        ax.set_xlim(-1.2, 1.2)\n",
    "        ax.set_ylim(-1.2, 1.2)\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(\"QPSK Constellation\")\n",
    "        \n",
    "        self.logger.experiment.add_figure(\"constellation\", fig, self.global_step)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx=None):\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            noisy_signal = batch[0]\n",
    "        else:\n",
    "            noisy_signal = batch\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.student(noisy_signal)\n",
    "            z = reparameterize(mu, logvar)\n",
    "            z_hat = self.denoiser(z)\n",
    "            z_hat = z_hat / (torch.norm(z_hat, dim=-1, keepdim=True) + 1e-8)\n",
    "            \n",
    "        return z_hat\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6ad97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    signal_params = dict(\n",
    "        time=2.0,\n",
    "        symbol_rate=6,\n",
    "        sample_rate=480,\n",
    "        carrying_freq=5.0\n",
    "    )\n",
    "\n",
    "    datamodule = QPSKDataModule(batch_size=128, num_workers=4, **signal_params)\n",
    "    model = QPSKDenoiserVSSL(ema_decay=0.995, lr=1e-3)\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=\"tb_logs\", name=\"qpsk_vssl\")\n",
    "    trainer = Trainer(\n",
    "        max_epochs=100,\n",
    "        logger=logger,\n",
    "        check_val_every_n_epoch=5,\n",
    "        accelerator='auto',\n",
    "        devices=1\n",
    "    )\n",
    "    trainer.fit(model, datamodule)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ec73d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model):\n",
    "    test_dataset = QPSKDataset(\n",
    "        num_samples=10,\n",
    "        augment_fn=qpsk_augmentation,\n",
    "        time=2.0,\n",
    "        symbol_rate=6,\n",
    "        sample_rate=480,\n",
    "        carrying_freq=5.0\n",
    "    )\n",
    "    \n",
    "    noisy_signals, _ = next(iter(DataLoader(test_dataset, batch_size=10)))\n",
    "    \n",
    "    model.eval()\n",
    "    denoised = model.predict_step(noisy_signals)\n",
    "    \n",
    "    for i in range(3):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax[0].scatter(noisy_signals[i,0], noisy_signals[i,1], alpha=0.5)\n",
    "        ax[0].set_title(f\"Noisy Signal {i+1}\")\n",
    "        ax[0].grid(True)\n",
    "        \n",
    "        ax[1].scatter(denoised[i,0], denoised[i,1], c='r')\n",
    "        ax[1].set_title(f\"Denoised Signal {i+1}\")\n",
    "        ax[1].grid(True)\n",
    "        ax[1].set_xlim(-1.1, 1.1)\n",
    "        ax[1].set_ylim(-1.1, 1.1)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model()\n",
    "visualize_results(trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
