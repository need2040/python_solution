{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"1. Логистическая_регрессия(colab).ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5mfvcFJ-fF3f"},"source":["# Логистическая регрессия. Нейронные сети\n","\n","Добро пожаловать в первую лабораторную работу по нейронным сетям. Зачастую первое, что пытается сделать исследователь со своим набором данных - это получить представление о классификации объектов в нем. Данное задание знакомит с меделью логистической регрессии - это алгоритм классификации, а не регрессии как можно подумать из названия! Данный алгоритм был разработан задолго до появления компьютеров, однако до сих под остается востребованным благодаря своей простоте и уникальности.\n","\n","**Указания к выполнению:**\n","- Не используйте циклы (for/while) в коде, если в задании не сказано об обратном.\n","\n","**Задачи лабораторной №2:**\n","- Создать основные методы для обучающения нейронной сети:\n","    - Инициализация параметров\n","    - Вычисление функции стоимости и ее градиента\n","    - Использование оптимизационного алгоритма (градиентный спуск)\n","- Обучить нейронную сеть"]},{"cell_type":"markdown","metadata":{"id":"QIQSaRj4fF3m"},"source":["Логистическая регрессия – хорошо известный вид классификации в линейном моделировании. Она применима как для бинарной классификации, так и для многоклассовой – в форме мультиномиальной логистической регрессии. Технически логистическая регрессия представляет собой регрессионную модель с категориальной зависимой переменной. Бинарная логистическая модель применяется для оценивания вероятности бинарного отклика на одну или несколько входных переменных (независимых переменных, или признаков). На выходе получается статистическая вероятность категории при условии заданных входных предикторов."]},{"cell_type":"markdown","metadata":{"id":"I7ndfAxsfF3o"},"source":["# 1. Подключение библиотек\n","\n","В начале работы необходимо импортировать все необходимые библиотеки для выполнения задания:\n","    - numpy - это библиотека для научных расчетов\n","    - h5py - \n","    - matplotlib\n","    - PIL и scipy"]},{"cell_type":"code","metadata":{"id":"7X_NcyM5fF3p","executionInfo":{"status":"ok","timestamp":1637691763888,"user_tz":-420,"elapsed":430,"user":{"displayName":"Владимир Спицын","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925311082516717393"}}},"source":["# import \"sample_data\\\\testCases\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy\n","from PIL import Image\n","from scipy import ndimage\n","from utils import load_dataset\n","\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FGSeDUZfF3u"},"source":["# 2. Анализ входных данных\n","\n","Входной датасет для задания состоит из наборов изображений, размеченных на два класса - \"кошки\" (y=1) и \"не кошки\" (y=0), и разделяется на:\n","* тренировочной набор изображений `m_train`, \n","* тестовый набор изображений `m_test`.\n","Каждое изображение размером `(num_py, num_px)` имеет 3 канала RGB.\n","\n","**Задание:** построить с помощью алгоритма логистической регрессии бинарный классификатор для классификации описанного выше набора изображений.\n","\n","Запустите код ниже, чтобы загрузить датасет с помощью вспомогательных функций, описанных в файле `utils.py`"]},{"cell_type":"code","metadata":{"id":"aakOMtKXfF3v"},"source":["# Загрузка наборов данных\n","train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VWlISH-fF35"},"source":["В датаесетах train_set_x_orig и test_set_x_orig каждая строка представляет собой изображение. Визуализируйте некоторые примеры, меняя значение index и выполняя код."]},{"cell_type":"code","metadata":{"id":"iPnZR7c4fF37"},"source":["# Пример изображения\n","index = 50\n","plt.imshow(train_set_x_orig[index])\n","print (\"y = \" + str(train_set_y[:, index]) + \", это класс '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \"' изображений.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Gq1sn2hfF3_"},"source":["index = 10\n","plt.imshow(train_set_x_orig[index])\n","print (\"y = \" + str(train_set_y[:, index]) + \", это класс '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \"' изображений.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rH6ZFFxlfF4J"},"source":["Найдите значения следующих парамеров датасета:\n","* `m_train` - число обучающих примеров\n","* `m_test` - число тестовых примеров\n","* `num_px` - ширина изображения\n","* `num_py` - высота изображения\n","\n","`train_set_x_orig` является numpy-тензором размером `(m_train, num_py, num_px, 3)`."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"4cf3b591546cc40d7697a2dad43dfa52","grade":false,"grade_id":"vector_shape","locked":false,"schema_version":1,"solution":true},"id":"dhvVw09ifF4N"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()\n","\n","print (\"Количество обучающих примеров: m_train = \" + str(m_train))\n","print (\"Количество тестовых примеров: m_test = \" + str(m_test))\n","print (\"Ширина изображения: num_px = \" + str(num_px))\n","print (\"Высота изображения: num_py = \" + str(num_py))\n","print (\"Форма массива изображения: (\" + str(num_py) + \", \" + str(num_px) + \", 3)\")\n","print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FcumlLG-fF4Z"},"source":["Сначала измените форму тензора изображения (num_py, num_px, 3) на numpy-тензор формой (num_py $*$ num_px $*$ 3, 1), где каждый столбец будет соотвествовать изображению.\n","\n","Преобразование вектора X размером (a,b,c,d) в матрицу X_flatten размером (b$*$c$*$d, a) можно осуществить следующим образом: \n","```python\n","X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n","```"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"8c5f8dd87f753840820b1deb15cc4149","grade":false,"grade_id":"reshape_array","locked":false,"schema_version":1,"solution":true},"id":"oRqyRNNTfF4a"},"source":["# Измените форму тренировочного и тестового набора\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()\n","\n","print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"1b8d1fb878ec3a545d8151f2347a2757","grade":true,"grade_id":"correct_shape","locked":true,"points":1,"schema_version":1,"solution":false},"id":"M1MJDk_LfF4e"},"source":["# Проверка правильности измнения формы массивов\n","assert np.array_equal(train_set_x_flatten[0:5,0],np.array([17, 31, 56, 22, 33]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2wyKiot3fF4h"},"source":["Для представления изображений в цветовой системе RGB значение каждого пикселя представляет собой вектор из трех чисел в диапазоне от 0 до 255.\n","\n","Было бы проблематично передать в нейронную сеть значения, имеющие самые разные диапазоны. Сеть, конечно, сможет автоматически адаптироваться к таким разнородным данным, однако это усложнит обучение. На практике Одним из распространенных шагов предварительной обработки в машинном обучении является нормализация: для каждого признака во входных данных \n","(столбца в матрице входных данных) из каждого значения вычитается среднее по этому признаку, и разность делится на стандартное отклонение, в результате признак центрируется по нулевому значению и имеет стандартное отклонение, равное единице. Такую нормализацию легко выполнить с помощью Numpy.\n","\n","Для нашего набора изображений просто разделим каждую строку набора данных на 255 (максимальное значение пиксельного канала)."]},{"cell_type":"code","metadata":{"id":"AdCH623NfF4l"},"source":["train_set_x = train_set_x_flatten/255.\n","test_set_x = test_set_x_flatten/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HvmWeiFDfF4v"},"source":["<font color='blue'>\n","**Промежуточные итоги:**\n","\n","Распространенными шагами предварительной обработки данных являются:\n","- Определение числа и исходных размеров входных данных (m_train, m_test, num_px, ...)\n","- Измение формы набора данных таким образом, чтобы каждый пример представлял собой вектор-столбец\n","- Нормализация данных"]},{"cell_type":"markdown","metadata":{"id":"mKwhtomwfF4w"},"source":["## 3 - Модель нейронной сети ##\n","\n","Многослойные персептроны (MLP) также называют простыми нейронными сетями прямого распространения, а иногда и просто нейронными сетями. MLP можно рассматривать как обобщение линейных моделей, которое прежде чем прийти к решению выполняет несколько этапов обработки данных. Следующее изображение объясняет, почему **Логистическая регрессия является очень простой нейронной сетью!**\n","\n","<img src=\"https://drive.google.com/uc?id=1K0yi6aYhxJZ4vy0sRnux1mJ_8MSl1lBN\" width=70%>\n","\n","**Математическая формулировка**:\n","\n","Для одного экземпляра обучения $x^{(i)}$:\n","$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n","$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n","$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n","\n","Функция стоимости рассчитывается путем суммирования всех стоимостей примеров обучения:\n","$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n","\n","**Задачи**:\n","В этом упражнении необходимо выполнить следующие шаги:\n","     - инициализировать параметры модели\n","     - изучить параметры модели, обучить модель, минимизировав стоимость\n","     - использовать обученную модель для прогнозирования (на тестовом наборе)\n","     - проанализировать результаты и сделать вывод"]},{"cell_type":"markdown","metadata":{"id":"ttvqIcSsfF4x"},"source":["## 4 - Построение алгоритма ## \n","\n","Основными шагами для для построения нейронной сети являются:\n","1. Определение архитектуры сети (число входных признаков, нейронов) \n","2. Инициализация параметров сети\n","3. Обучение в виде цикла:\n","    - Рассчет ошибки итерации (прямое распространение ошибки)\n","    - Расчет градиента ошибки (обатное распространение ошибки)\n","    - Обновление параметров сети (градиентный спуск)\n","\n","Вам необходимо реализовать пункты 1-3 отдельными функциями и затем соединить их в функции `model()`.\n","\n","### 4.1 - Сигмоидальная функция активации\n","\n","**Задание**: Реализуйте сигмоидальную функцию `sigmoid()`, вычисляющую $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$. Используйте np.exp()."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"96c0f93f10da1ad0a32375a18b27670b","grade":false,"grade_id":"sigmoid","locked":false,"schema_version":1,"solution":true},"id":"1qmJlAG0fF4y"},"source":["# Функция активации: sigmoid\n","\n","def sigmoid(z):\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"c6bb84230c3b1390d6c4e6e0505beb9e","grade":true,"grade_id":"correct_sigmoid","locked":true,"points":2,"schema_version":1,"solution":false},"id":"6DhSl9V6fF48"},"source":["assert sigmoid(np.array([0,2]))[0] == 0.5\n","assert round(sigmoid(np.array([0,2]))[1], 6) == 0.880797"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3S9NIkhfF5B"},"source":["### 4.2 - Инициализация параметров\n","**Задание:** Реализуйте инициализацию весовых коэффициентов нулями, используя np.zeros()."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"9565d9580b89280ece77124456155cc4","grade":false,"grade_id":"init_func","locked":false,"schema_version":1,"solution":true},"id":"C3dSwKwKfF5C"},"source":["# Инициализация нулями: initialize_with_zeros\n","\n","def initialize_with_zeros(dim):\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"b3448ad7213769872941eeeed20143c9","grade":true,"grade_id":"correct_init","locked":true,"points":2,"schema_version":1,"solution":false},"id":"uR-oBDj2fF5Q"},"source":["dim = 2\n","w, b = initialize_with_zeros(dim)\n","assert np.array_equal(w,np.array([[0.],[0.]]))\n","assert np.array_equal(b,0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPPxXVVmfF5b"},"source":["### 4.3 - Прямое и обратное распространение ошибки\n","\n","Наиболее известный алгоритм обучения нейронной сети - метод обратного распространения ошибки. \n","Данный алгоритм используется для минимизации отклонения реальных значений выходных сигналов нейронной сети от требуемых. \n","\n","**Задание:** Реализуйте функцию `propagate()`, которая вычисляет функцию стоимости и ее градиент.\n","\n","**Подсказки**:\n","\n","Прямое распространение сигнала:\n","- Предъявление очередного экземпляра Х из обучающей выборки на вход нейронной сети\n","- Вычисление выходного сигнала $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n","- Определение величин ошибок нейронов выходного слоя: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n","\n","Формулы для обратного распространения ошибки: \n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"1aa2aa0ccda5710f38fb7b2035611e2d","grade":false,"grade_id":"prop","locked":false,"schema_version":1,"solution":true},"id":"ux-qFPrsfF5c"},"source":["# Прямое и обратное распространение ошибки: propagate\n","\n","def propagate(w, b, X, Y):\n","    m = X.shape[1]\n","    \n","    # FORWARD PROPAGATION (FROM X TO COST)\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"bd58c12e3fcd7364ce4505812d1c95c7","grade":true,"grade_id":"correct_prop","locked":true,"points":3,"schema_version":1,"solution":false},"id":"42QJ8GWnfF5g"},"source":["w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\n","grads, cost = propagate(w, b, X, Y)\n","\n","assert np.round(grads[\"dw\"])[0] == 1\n","assert np.round(grads[\"dw\"])[1] == 2\n","assert np.round(grads[\"db\"],1) == 0.5\n","assert np.round(cost) == 6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7idGHIozfF5j"},"source":["### 4.4 - Оптимизация\n","\n","**Задание:** Напишите ниже функцию обучения, которая оптимизирует веса $w$ и смещения $b$ для минимизации функции ошибки выхода $J$. Для параметра $\\theta$ реализуйте правило обновления $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, где $\\alpha$ - скорость обучения."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"46c6cb59242844c34ab42b9e14290dc1","grade":false,"grade_id":"optimization","locked":false,"schema_version":1,"solution":true},"id":"ltjcakkLfF5k"},"source":["# Функция оптимизации: optimize\n","\n","def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        \n","        \n","        # Вычисление градиента и функции стоимости\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","        \n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # обновление весов\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","        \n","        if i % 100 == 0:\n","            costs.append(cost)\n","        \n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"5ab62121f96f875f096d2d7d222e452c","grade":true,"grade_id":"correct_optimize","locked":true,"points":4,"schema_version":1,"solution":false},"id":"uFOaelFJfF5w"},"source":["params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n","\n","pw = params[\"w\"]\n","pb = params[\"b\"]\n","assert np.round(pw,5)[0] == 0.11246\n","assert np.round(pw,5)[1] == 0.23107\n","assert np.round(pb,5) == 1.55930\n","\n","assert np.round(grads[\"dw\"],5)[0] == 0.90158\n","assert np.round(grads[\"dw\"],5)[1] == 1.76251\n","assert np.round(grads[\"db\"],5) == 0.43046"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYxuRVIJfF50"},"source":["**Задание:** Предыдущая функция обучения позволяет подобрать параметры весов w и смещений b нейронов сети. Используйте обученные параметры для предсказания класса экземпляра входных данных X. Реализуйте функцию `predict()`, состоящую из следующих шагов:\n","\n","1. Вычисление $\\hat{Y} = A = \\sigma(w^T X + b)$\n","\n","2. Присвоение метки класса для значения a: если значение функции активации <= 0.5, то 0, иначе 1 (активация > 0.5). запись результата классификации в вектор `Y_prediction`. "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"84e790350d33476a3a7230fd60b3381b","grade":false,"grade_id":"predict","locked":false,"schema_version":1,"solution":true},"id":"IxxdiihQfF51"},"source":["# Функция классификации: predict\n","\n","def predict(w, b, X):\n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    \n","    # Вычислите вектор \"A\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    for i in range(A.shape[1]):\n","        \n","        # Произведите классификацию с помощью порогового значения 0.5\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"bacb588303d6f681b1b60cd692c5e722","grade":true,"grade_id":"correct_predict","locked":true,"points":1,"schema_version":1,"solution":false},"id":"VpW_WRPyfF58"},"source":["assert np.array_equal(predict(w, b, X),np.array([[1.,1.]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qgGxzPr-fF6A"},"source":["<font color='blue'>\n","**Промежуточные итоги:**\n","\n","Основными шагами для классификации нейронной сетью являются:\n","* Инициализация начальных значений параметров сети (w,b)\n","* Итерационная оптимизация функции ошибки путем обучения параметров (w,b):\n","  *   вычисление функции стоимости и ее градиента\n","  *   обновление значений параметров используя метод градиентного спуска\n","* Классификация набора данных с использованием обученных (w,b)"]},{"cell_type":"markdown","metadata":{"id":"herFW7FifF6B"},"source":["## 5 - Создание классификатора ##\n","\n","После того, как реализованы все базовые функции, создадим классификатор на основе неронной сети.\n","\n","**Задание:** Реализуйте функцию `model()`:\n","    - Y_prediction_test используйте для тестирования модели на тестовом наборе\n","    - Y_prediction_train используйте для тестирования модели на тренировочном наборе\n","    - w, costs, grads - результат фнкции optimize()"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"545c8ee959b167c56c85544a370e0542","grade":false,"grade_id":"model","locked":false,"schema_version":1,"solution":true},"id":"OijpMXXLfF6C"},"source":["# Модель классификатора: model\n","\n","def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","\n","    # инициализация параметов (w, b)\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    # Градиентный спуск\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","        \n","    # Предсказание Y_prediction_test, Y_prediction_train \n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","\n","    \n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test, \n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","    \n","    return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSLFZGBKfF6H"},"source":["# Обучим полученную модель\n","d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfkiNuplfF6O"},"source":["**Примерные результаты**: \n","\n","<table style=\"width:40%\"> \n","\n","    <tr>\n","        <td> **Ошибка после 0 итерации **  </td> \n","        <td> 0.693147 </td>\n","    </tr>\n","      <tr>\n","        <td> <center> $\\vdots$ </center> </td> \n","        <td> <center> $\\vdots$ </center> </td> \n","    </tr>  \n","    <tr>\n","        <td> **Точность на обучающей выборке**  </td> \n","        <td> 99.04306220095694 % </td>\n","    </tr>\n","\n","    <tr>\n","        <td>**Точность на тестовой выборке** </td> \n","        <td> 70.0 % </td>\n","    </tr>\n","</table> \n"]},{"cell_type":"markdown","metadata":{"id":"zkQA3PFAfF6Q"},"source":["** Комментарий **: точность обучения модели на тренировочном модели стремиться к 100%, что означает, что модель работает и имеет достаточно хорошее соответствие данным обучения. Точность реализованного классификатора на тестовом наборе приблизительно составляет 68%, что неплохо для такой линейной модели, как логистическая регрессия, а также обученной на маленьком наборе данных. В последующих лабоаторных работах Вам предстоить усложнить нейронную сеть и создать более точный классификатор!\n","\n","Позже Вы узнаете, как бороться с переобучением, например, с помощью регуляризации, а пока можете проверить результаты обучения модели, используя приведенный ниже код (и изменяя переменную `index`). Посмотрите результаты классификации на изображениях тестового набора."]},{"cell_type":"code","metadata":{"id":"ByK68lW2fF6R"},"source":["index = 6\n","plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n","print (\"y = \" + str(test_set_y[0,index]) + \", предсказание нейросети: \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])].decode(\"utf-8\") +  \"\\\" на изображении.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mm-T2iEfF6V"},"source":["Построим график обучения сети: график зависимости функции стоимости от времени обучения "]},{"cell_type":"code","metadata":{"id":"Vo1ObShFfF6X"},"source":["costs = np.squeeze(d['costs'])\n","plt.plot(costs)\n","plt.ylabel('ошибка')\n","plt.xlabel('число итераций обучения')\n","plt.title(\"Скорость обучения =\" + str(d[\"learning_rate\"]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I92lJ0BOfF6d"},"source":["Пояснение: график потерь в результате обучения стремиться к 0, что говорит о том, что модель обучается и все больше соотвествует данным. Однако с увеличением количества эпох обучения модель начинает подстраиваться под обучающие данные, \"запоминать\" тренировочный набор, однако это приводит зачастую к тому, что точность на тестовом наборе уменьшается. Выражаясь точнее, происходит переобучение: чрезмерная оптимизация на обучающих данных, что приводит к представлению, характерному для обучающих данных, не обобщающее данные за пределами обучающего набора. \n","\n","**Задание**: Попробуйте увеличить количество итераций модели и перезапустите код обучения. Сравните точность обучения на тренировочном и тестовом наборе, сделайте выводы в ячейке ниже."]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"checksum":"c10aace0cc691684daca7cda29323c7b","grade":true,"grade_id":"overfit","locked":false,"points":1,"schema_version":1,"solution":true},"id":"zDBbPAuAfF6f"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"Mu5vBsT8fF6g"},"source":["## 6 - Настройка гиперпараметров ##\n","\n","Установите экспериментально значение параметра скорости обучения модели, протестировав обучение с различными $\\alpha$."]},{"cell_type":"markdown","metadata":{"id":"5_L2ArS7fF6h"},"source":["#### Выбор скорости обучения ####\n","\n","** Пояснение **: для того, чтобы метод градиентого спуска приводил к оптимизации модели, а не расходился или застревал в локальных минимумах, следует осознано подбирать параметр скорости обучения. Скорость обучения $ \\ alpha $ определяет, насколько быстро мы обновляем параметры. Если скорость обучения слишком велика, градиентный спуск может «перепрыгнуть» оптимальное значение. Точно так же, если скорость слишком мала, для обучения потребуется огромное число итераций. Вот почему важно обращать внимание на скорость обучения.\n","\n","Сравните кривую обучения модели с несколькими вариантами скорости обучения и сделайте выводы. Не стесняйтесь также попробовать значения, отличные от заданных для переменной `learning_rates`."]},{"cell_type":"code","metadata":{"id":"uNJ8plIdfF6i"},"source":["learning_rates = [0.01, 0.001, 0.0001]\n","models = {}\n","for i in learning_rates:\n","    print (\"learning rate is: \" + str(i))\n","    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n","    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n","\n","for i in learning_rates:\n","    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n","\n","plt.ylabel('ошибка')\n","plt.xlabel('число итераций обучения')\n","\n","legend = plt.legend(loc='upper center', shadow=True)\n","frame = legend.get_frame()\n","frame.set_facecolor('0.90')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9emyVQwvfF6m"},"source":["Напишите выводы о результатах обучения с различными значениями скорости обучения"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"checksum":"c5bd4de50ed0b8db2c80047e2255eb6f","grade":true,"grade_id":"lening_rate","locked":false,"points":1,"schema_version":1,"solution":true},"id":"2GQbVdq-fF6n"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"y2CpW3TIfF6p"},"source":["## 7 - Тестирование произвольных изображений ##\n","\n","Вы можете использовать произвольные изображения для того, чтобы оценить полученную нейронную сеть. Для этого:\n","    1. Загрузите изображение в директрию images данного Jupyter Notebook\n","    3. Измените переменную my_name на имя загруженного изображения\n","    4. Запустите ячейку кода и оцените результат (1 = cat, 0 = non-cat)!"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"4c90370e116ae5045cb90414e59a341a","grade":false,"grade_id":"test_image","locked":false,"schema_version":1,"solution":true},"id":"2zI0Y4xNfF6u"},"source":["from skimage import transform\n","from skimage.io import imread\n","\n","\"\"\"my_image = Укажите имя вашего изображения в папке images\"\"\"\n","# YOUR CODE HERE\n","raise NotImplementedError()\n","\n","# Загрузка указанного изображения\n","fname = \"images/\" + my_image\n","image = np.array(imread(fname))\n","\n","# Предобработка и изменение размера изображения\n","image = image/255.\n","image = transform.resize(image, [num_py,num_px,3])\n","my_image = image.reshape((1, num_py*num_px*3)).T\n","\n","my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n","\n","plt.imshow(image)\n","print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", предсказание нейросети: \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" на изображении.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-kU5iqI3d8sH"},"source":["Контрольные вопросы:\n","1. Что такое логистическая регрессия?\n","2. Что такое нейронная сеть?"]}]}